{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Classification Based on Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch Device 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu126\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "  my_device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "  my_device = torch.device('cuda')\n",
    "else:\n",
    "  my_device = torch.device('cpu')\n",
    "\n",
    "print(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../rnn/name_gender_filtered.csv')\n",
    "\n",
    "unique_chars = set()\n",
    "\n",
    "for name in df['Name']:\n",
    "  unique_chars.update(name)\n",
    "\n",
    "# set to sorted list\n",
    "sorted_chars = sorted(list(unique_chars))\n",
    "print(sorted_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stoi, itos dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '<P>': 26}\n",
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '<P>'}\n"
     ]
    }
   ],
   "source": [
    "# string(char) to index\n",
    "stoi = {s:i for i, s in enumerate(sorted_chars)}\n",
    "# padding token\n",
    "stoi['<P>'] = len(stoi)\n",
    "\n",
    "# index to string(char)  (with padding token)\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encode, decode function\n",
    "\n",
    "1. encode function : string to indices list : 최대 문자 길이를 맞추기 위해 padding<br>\n",
    "2. decode function : indices to string list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 14, 2, 14, 15, 4, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
      "nocope\n"
     ]
    }
   ],
   "source": [
    "# maximume word's char length\n",
    "char_length = 16\n",
    "\n",
    "# @param {string} name : string name\n",
    "def encode_name(name):\n",
    "  encoded_name = [stoi[s] for s in name]\n",
    "  encoded_name += [stoi['<P>']] * (char_length-len(name))\n",
    "  return encoded_name\n",
    "\n",
    "# @param {index list} name : index list\n",
    "def decode_name(name):\n",
    "  # remove the padding token\n",
    "  decoded_name = [itos[i] for i in name if itos[i] != '<P>']\n",
    "  decoded_name = ''.join(decoded_name)\n",
    "  return decoded_name\n",
    "\n",
    "\n",
    "print(encode_name('nocope'))\n",
    "print(decode_name(encode_name('nocope')))\n",
    "\n",
    "\n",
    "gen2num = {'F': 0, 'M': 1}\n",
    "num2gen = {0: 'F', 1: 'M'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer for genger classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character dict length: 27\n"
     ]
    }
   ],
   "source": [
    "n_name_max_length = 16\n",
    "n_embed_dim = 32\n",
    "n_multi_heads = 4\n",
    "n_layers = 4\n",
    "\n",
    "# charactor dict length\n",
    "n_char_dict_length = len(stoi)\n",
    "n_embeddings = n_char_dict_length\n",
    "\n",
    "print(f\"character dict length: {n_char_dict_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from xd_selfattention import XD_TransformerBlock\n",
    "\n",
    "class XD_GenderTransformer(nn.Module):\n",
    "  def __init__(self, name_max_length, embed_dim, num_heads, num_layers, num_embeddings, num_classes=2)\n",
    "    super.__init__()\n",
    "\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    # Embedding : 정수 인덱스를 연속적인 임베딩벡터로 변환\n",
    "    # num_embeddings : 임베딩할 고유 인덱스의 개수 (예: 단언 사전 크기)\n",
    "    # embed_dim : 각 인덱스를 나타낼 임베딩 벡터 차원\n",
    "    self.char_embedding = nn.Embedding(num_embeddings, embed_dim)\n",
    "\n",
    "    # Positional Encoding\n",
    "    self.positional_encoding = nn.Embedding(name_max_length, embed_dim)\n",
    "\n",
    "    # Transformer Block : layer 개수 만큼 transform block 을 생성\n",
    "    self.transformer_blocks = nn.Sequential([XD_TransformerBlock(n_embed_dim, n_multi_heads) for _ in range(n_layers)])\n",
    "\n",
    "    # last layer Normalization\n",
    "    self.last_lnorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    # Classifier : embedding dim -> classes count\n",
    "    self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "  \n",
    "  def forword(self, x):\n",
    "    # name's char embedding vector (with char unit)\n",
    "    # char_embeddings = [batch_size, seq_length, embed_dim]\n",
    "    char_embeddings = self.char_embedding(x)  \n",
    "\n",
    "    # name's char poisition\n",
    "    # char_positions = [1, name_length] \n",
    "    char_positions = torch.arrange(0, x.size(1), device=my_device).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
