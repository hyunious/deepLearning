{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Based Country Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding 을 위해 사용된 문자셋을 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Count: 18, Countries=['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n",
      "[('Arabic', 2000), ('Chinese', 268), ('Czech', 519), ('Dutch', 297), ('English', 3668), ('French', 277), ('German', 724), ('Greek', 203), ('Irish', 232), ('Italian', 709), ('Japanese', 991), ('Korean', 94), ('Polish', 139), ('Portuguese', 74), ('Russian', 9408), ('Scottish', 100), ('Spanish', 298), ('Vietnamese', 73)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('name_country.csv')\n",
    "\n",
    "# 이름 리스트를 얻어옴\n",
    "name_data = df['Name'].to_list()\n",
    "# 국적 데이터를 얻어옴 \n",
    "country_data = df['Country'].to_list()\n",
    "\n",
    "# 국적 리스트를 구성 : set 을 이용하여 중복 제거 후 정렬\n",
    "country_list = sorted(set(country_data))\n",
    "country_count = len(country_list)\n",
    "print(f\"Country Count: {country_count}, Countries={country_list}\")\n",
    "\n",
    "# 국적 to 인덱스로 변환\n",
    "country_to_index = {country: i for i, country in enumerate(country_list)}\n",
    "print(f\"Country Index: {country_to_index}\")\n",
    "\n",
    "# collections.Counter를 사용하여 국적별 데이터 수 계산\n",
    "country_counts = Counter(country_data)\n",
    "print(sorted(country_counts.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Character Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character count: 28, characters= 'abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding 을 위한 문자 집합 생성\n",
    "unique_chars = set()\n",
    "\n",
    "# set 집합에 문자열을 추가하면 해당 문자열을 낱개로 쪼개어 각각의 문자들을 하나의 인자로 인식하여 집합에 추가\n",
    "# 중복된 문자는 추가되지 않음.!!!\n",
    "for name in name_data:\n",
    "    unique_chars.update(name)\n",
    "    if ',' in name:\n",
    "        print(f\"쉼표가 포함된 이름 발견: {name}\")\n",
    "\n",
    "# 문자 집합을 정렬  \n",
    "unique_chars = sorted(list(unique_chars))\n",
    "unique_chars = ''.join(unique_chars)\n",
    "print(f\"character count: {len(unique_chars)}, characters={unique_chars}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Name to One-Hot Encoded Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_letters = len(unique_chars)\n",
    "\n",
    "def name_to_tensor(name):\n",
    "    tensor = torch.zeros(len(name), n_letters)\n",
    "    for i, letter in enumerate(name):\n",
    "        letter_index = unique_chars.find(letter)\n",
    "        assert letter_index != -1, \"letter not found: \" + letter\n",
    "        tensor[i][letter_index] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xd_rnn import XD_RNN\n",
    "\n",
    "# 은닉층 수\n",
    "n_hidden = 32\n",
    "# 입력층 수, 은닉층 수, 출력층 수\n",
    "rnn_model = XD_RNN(n_letters, n_hidden, country_count)\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.001\n",
    "# 학습 횟수\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.5712, Accuracy: 82.270599%\n",
      "Epoch 2/200, Loss: 0.5672, Accuracy: 82.300488%\n",
      "Epoch 3/200, Loss: 0.5673, Accuracy: 82.235728%\n",
      "Epoch 4/200, Loss: 0.5694, Accuracy: 82.320414%\n",
      "Epoch 5/200, Loss: 0.5650, Accuracy: 82.310451%\n",
      "Epoch 6/200, Loss: 0.5688, Accuracy: 82.041447%\n",
      "Epoch 7/200, Loss: 0.5631, Accuracy: 82.151041%\n",
      "Epoch 8/200, Loss: 0.5680, Accuracy: 82.350304%\n",
      "Epoch 9/200, Loss: 0.5658, Accuracy: 82.370230%\n",
      "Epoch 10/200, Loss: 0.5649, Accuracy: 82.345322%\n",
      "Epoch 11/200, Loss: 0.5667, Accuracy: 82.300488%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m loss = loss_fn(output, target_tensor)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# 손실 역전파\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m loss.backward()\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# 최적화 실행\u001b[39;00m\n\u001b[32m     50\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m torch.autograd.backward(\n\u001b[32m    523\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    524\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    267\u001b[39m     tensors,\n\u001b[32m    268\u001b[39m     grad_tensors_,\n\u001b[32m    269\u001b[39m     retain_graph,\n\u001b[32m    270\u001b[39m     create_graph,\n\u001b[32m    271\u001b[39m     inputs,\n\u001b[32m    272\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    273\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    274\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# 최적화 알고리즘\n",
    "optimizer = Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 손실 함수\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 모델 학습 설정\n",
    "rnn_model.train()\n",
    "\n",
    "\n",
    "# 학습 횟수만큼 반복\n",
    "for epoch in range(epochs):\n",
    "    # 데이터 셔플 - reference : https://blog.naver.com/frogsom1120/222127699322\n",
    "    shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # 데이터 분할은 하지 않음.\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # 데이터 (rows) 학습 \n",
    "    for index, row in shuffled_df.iterrows():\n",
    "        # 이름을 텐서로 변환 (one-hot encoding)\n",
    "        input_tensor = name_to_tensor(row['Name'])\n",
    "        # 국적을 텐서로 변환\n",
    "        target_tensor = torch.tensor([country_list.index(row['Country'])], dtype=torch.long)\n",
    "\n",
    "        # 모델 은닉층(상태)를 얻어옴\n",
    "        hidden = rnn_model.get_hidden()\n",
    "\n",
    "        # 모델 그레디언트 초기화\n",
    "        rnn_model.zero_grad()\n",
    "\n",
    "        # rnn 학습\n",
    "        for char_index in range(input_tensor.size(0)):\n",
    "            # char tensor 추출 : 2차원 텐서 (1, 28)\n",
    "            char_tensor = input_tensor[char_index]\n",
    "            # name char 학습 : 1차원 텐서 (28)\n",
    "            output, hidden = rnn_model(char_tensor[None, :], hidden)\n",
    "\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = loss_fn(output, target_tensor)\n",
    "        # 손실 역전파\n",
    "        loss.backward()\n",
    "        # 최적화 실행\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 합계 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 예측 결과 계산\n",
    "        predicted_index = torch.argmax(output, dim=1)\n",
    "\n",
    "        # 예측 결과 확인\n",
    "        correct_predictions += (predicted_index == target_tensor).sum().item()\n",
    "        total_predictions += 1\n",
    "\n",
    "\n",
    "    # 평균 손실 계산\n",
    "    avg_loss = total_loss / total_predictions\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "    # 학습 횟수 출력\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:2f}%\")\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n"
     ]
    }
   ],
   "source": [
    "test_name = 'jinping'\n",
    "test_tensor = name_to_tensor(test_name)\n",
    "\n",
    "rnn_model.eval()\n",
    "\n",
    "hidden = rnn_model.get_hidden()\n",
    "\n",
    "for char_index in range(test_tensor.size(0)):\n",
    "    char_tensor = test_tensor[char_index]\n",
    "    output, hidden = rnn_model(char_tensor[None, :], hidden)\n",
    "\n",
    "\n",
    "# 예측 결과 확인\n",
    "predicted_index = torch.argmax(output, dim=1)\n",
    "print(country_list[predicted_index.item()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
