{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Based Country Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding 을 위해 사용된 문자셋을 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Count: 18, Countries=['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n",
      "Country Index: {'Arabic': 0, 'Chinese': 1, 'Czech': 2, 'Dutch': 3, 'English': 4, 'French': 5, 'German': 6, 'Greek': 7, 'Irish': 8, 'Italian': 9, 'Japanese': 10, 'Korean': 11, 'Polish': 12, 'Portuguese': 13, 'Russian': 14, 'Scottish': 15, 'Spanish': 16, 'Vietnamese': 17}\n",
      "[('Arabic', 2000), ('Chinese', 268), ('Czech', 519), ('Dutch', 297), ('English', 3668), ('French', 277), ('German', 724), ('Greek', 203), ('Irish', 232), ('Italian', 709), ('Japanese', 991), ('Korean', 94), ('Polish', 139), ('Portuguese', 74), ('Russian', 9408), ('Scottish', 100), ('Spanish', 298), ('Vietnamese', 73)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('name_country.csv')\n",
    "\n",
    "# 이름 리스트를 얻어옴\n",
    "name_data = df['Name'].to_list()\n",
    "# 국적 데이터를 얻어옴 \n",
    "country_data = df['Country'].to_list()\n",
    "\n",
    "# 국적 리스트를 구성 : set 을 이용하여 중복 제거 후 정렬\n",
    "country_list = sorted(set(country_data))\n",
    "country_count = len(country_list)\n",
    "print(f\"Country Count: {country_count}, Countries={country_list}\")\n",
    "\n",
    "# 국적 to 인덱스로 변환\n",
    "country_to_index = {country: i for i, country in enumerate(country_list)}\n",
    "print(f\"Country Index: {country_to_index}\")\n",
    "\n",
    "# collections.Counter를 사용하여 국적별 데이터 수 계산\n",
    "country_counts = Counter(country_data)\n",
    "print(sorted(country_counts.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Character Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character count: 28, characters= 'abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding 을 위한 문자 집합 생성\n",
    "unique_chars = set()\n",
    "\n",
    "# set 집합에 문자열을 추가하면 해당 문자열을 낱개로 쪼개어 각각의 문자들을 하나의 인자로 인식하여 집합에 추가\n",
    "# 중복된 문자는 추가되지 않음.!!!\n",
    "for name in name_data:\n",
    "    unique_chars.update(name)\n",
    "    if ',' in name:\n",
    "        print(f\"쉼표가 포함된 이름 발견: {name}\")\n",
    "\n",
    "# 문자 집합을 정렬  \n",
    "unique_chars = sorted(list(unique_chars))\n",
    "unique_chars = ''.join(unique_chars)\n",
    "print(f\"character count: {len(unique_chars)}, characters={unique_chars}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Name to One-Hot Encoded Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "n_letters = len(unique_chars)\n",
    "\n",
    "def name_to_tensor(name):\n",
    "    tensor = torch.zeros(len(name), n_letters)\n",
    "    for i, letter in enumerate(name):\n",
    "        letter_index = unique_chars.find(letter)\n",
    "        assert letter_index != -1, \"letter not found: \" + letter\n",
    "        tensor[i][letter_index] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xd_rnn import XD_RNN\n",
    "\n",
    "# 은닉층 수\n",
    "n_hidden = 32\n",
    "# 입력층 수, 은닉층 수, 출력층 수\n",
    "rnn_model = XD_RNN(n_letters, n_hidden, country_count)\n",
    "\n",
    "# 학습률\n",
    "learning_rate = 0.001\n",
    "# 학습 횟수\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 1.1350, Accuracy: 66.444157%\n",
      "Epoch 2/200, Loss: 0.8976, Accuracy: 73.288831%\n",
      "Epoch 3/200, Loss: 0.8155, Accuracy: 75.460795%\n",
      "Epoch 4/200, Loss: 0.7601, Accuracy: 76.950284%\n",
      "Epoch 5/200, Loss: 0.7229, Accuracy: 77.946598%\n",
      "Epoch 6/200, Loss: 0.6948, Accuracy: 78.703796%\n",
      "Epoch 7/200, Loss: 0.6757, Accuracy: 78.853243%\n",
      "Epoch 8/200, Loss: 0.6613, Accuracy: 79.505828%\n",
      "Epoch 9/200, Loss: 0.6461, Accuracy: 79.944206%\n",
      "Epoch 10/200, Loss: 0.6400, Accuracy: 80.098635%\n",
      "Epoch 11/200, Loss: 0.6323, Accuracy: 80.342732%\n",
      "Epoch 12/200, Loss: 0.6230, Accuracy: 80.571884%\n",
      "Epoch 13/200, Loss: 0.6189, Accuracy: 80.855833%\n",
      "Epoch 14/200, Loss: 0.6105, Accuracy: 81.005280%\n",
      "Epoch 15/200, Loss: 0.6101, Accuracy: 81.084986%\n",
      "Epoch 16/200, Loss: 0.6084, Accuracy: 81.025207%\n",
      "Epoch 17/200, Loss: 0.6009, Accuracy: 81.194580%\n",
      "Epoch 18/200, Loss: 0.5990, Accuracy: 81.368935%\n",
      "Epoch 19/200, Loss: 0.5993, Accuracy: 81.398824%\n",
      "Epoch 20/200, Loss: 0.5961, Accuracy: 81.533327%\n",
      "Epoch 21/200, Loss: 0.5927, Accuracy: 81.398824%\n",
      "Epoch 22/200, Loss: 0.5901, Accuracy: 81.518382%\n",
      "Epoch 23/200, Loss: 0.5902, Accuracy: 81.608050%\n",
      "Epoch 24/200, Loss: 0.5847, Accuracy: 81.737571%\n",
      "Epoch 25/200, Loss: 0.5871, Accuracy: 81.558235%\n",
      "Epoch 26/200, Loss: 0.5856, Accuracy: 81.732589%\n",
      "Epoch 27/200, Loss: 0.5826, Accuracy: 81.872073%\n",
      "Epoch 28/200, Loss: 0.5816, Accuracy: 81.727608%\n",
      "Epoch 29/200, Loss: 0.5750, Accuracy: 82.106207%\n",
      "Epoch 30/200, Loss: 0.5804, Accuracy: 81.772442%\n",
      "Epoch 31/200, Loss: 0.5769, Accuracy: 81.867092%\n",
      "Epoch 32/200, Loss: 0.5747, Accuracy: 82.131115%\n",
      "Epoch 33/200, Loss: 0.5762, Accuracy: 81.921889%\n",
      "Epoch 34/200, Loss: 0.5730, Accuracy: 82.295507%\n",
      "Epoch 35/200, Loss: 0.5730, Accuracy: 81.946797%\n",
      "Epoch 36/200, Loss: 0.5803, Accuracy: 81.717645%\n",
      "Epoch 37/200, Loss: 0.5762, Accuracy: 82.026502%\n",
      "Epoch 38/200, Loss: 0.5703, Accuracy: 81.976686%\n",
      "Epoch 39/200, Loss: 0.5701, Accuracy: 82.061373%\n",
      "Epoch 40/200, Loss: 0.5685, Accuracy: 82.146060%\n",
      "Epoch 41/200, Loss: 0.5707, Accuracy: 82.121152%\n",
      "Epoch 42/200, Loss: 0.5688, Accuracy: 82.295507%\n",
      "Epoch 43/200, Loss: 0.5651, Accuracy: 82.165986%\n",
      "Epoch 44/200, Loss: 0.5677, Accuracy: 82.350304%\n",
      "Epoch 45/200, Loss: 0.5691, Accuracy: 82.185912%\n",
      "Epoch 46/200, Loss: 0.5664, Accuracy: 82.161004%\n",
      "Epoch 47/200, Loss: 0.5622, Accuracy: 82.390156%\n",
      "Epoch 48/200, Loss: 0.5639, Accuracy: 82.385175%\n",
      "Epoch 49/200, Loss: 0.5644, Accuracy: 82.180931%\n",
      "Epoch 50/200, Loss: 0.5604, Accuracy: 82.210820%\n",
      "Epoch 51/200, Loss: 0.5629, Accuracy: 82.415064%\n",
      "Epoch 52/200, Loss: 0.5648, Accuracy: 82.260636%\n",
      "Epoch 53/200, Loss: 0.5592, Accuracy: 82.459898%\n",
      "Epoch 54/200, Loss: 0.5608, Accuracy: 82.584438%\n",
      "Epoch 55/200, Loss: 0.5573, Accuracy: 82.479825%\n",
      "Epoch 56/200, Loss: 0.5568, Accuracy: 82.454917%\n",
      "Epoch 57/200, Loss: 0.5582, Accuracy: 82.430009%\n",
      "Epoch 58/200, Loss: 0.5587, Accuracy: 82.674106%\n",
      "Epoch 59/200, Loss: 0.5569, Accuracy: 82.395138%\n",
      "Epoch 60/200, Loss: 0.5581, Accuracy: 82.479825%\n",
      "Epoch 61/200, Loss: 0.5605, Accuracy: 82.539603%\n",
      "Epoch 62/200, Loss: 0.5583, Accuracy: 82.664143%\n",
      "Epoch 63/200, Loss: 0.5575, Accuracy: 82.340341%\n",
      "Epoch 64/200, Loss: 0.5614, Accuracy: 82.479825%\n",
      "Epoch 65/200, Loss: 0.5580, Accuracy: 82.395138%\n",
      "Epoch 66/200, Loss: 0.5639, Accuracy: 82.474843%\n",
      "Epoch 67/200, Loss: 0.5567, Accuracy: 82.390156%\n",
      "Epoch 68/200, Loss: 0.5671, Accuracy: 82.385175%\n",
      "Epoch 69/200, Loss: 0.5601, Accuracy: 82.499751%\n",
      "Epoch 70/200, Loss: 0.5581, Accuracy: 82.624290%\n",
      "Epoch 71/200, Loss: 0.5593, Accuracy: 82.584438%\n",
      "Epoch 72/200, Loss: 0.5563, Accuracy: 82.723921%\n",
      "Epoch 73/200, Loss: 0.5578, Accuracy: 82.624290%\n",
      "Epoch 74/200, Loss: 0.5645, Accuracy: 82.350304%\n",
      "Epoch 75/200, Loss: 0.5617, Accuracy: 82.524659%\n",
      "Epoch 76/200, Loss: 0.5518, Accuracy: 82.664143%\n",
      "Epoch 77/200, Loss: 0.5545, Accuracy: 82.474843%\n",
      "Epoch 78/200, Loss: 0.5605, Accuracy: 82.300488%\n",
      "Epoch 79/200, Loss: 0.5610, Accuracy: 82.265617%\n",
      "Epoch 80/200, Loss: 0.5599, Accuracy: 82.519677%\n",
      "Epoch 81/200, Loss: 0.5575, Accuracy: 82.584438%\n",
      "Epoch 82/200, Loss: 0.5604, Accuracy: 82.694032%\n",
      "Epoch 83/200, Loss: 0.5530, Accuracy: 82.659161%\n",
      "Epoch 84/200, Loss: 0.5538, Accuracy: 82.589419%\n",
      "Epoch 85/200, Loss: 0.5611, Accuracy: 82.579456%\n",
      "Epoch 86/200, Loss: 0.5565, Accuracy: 82.564511%\n",
      "Epoch 87/200, Loss: 0.5577, Accuracy: 82.086281%\n",
      "Epoch 88/200, Loss: 0.5558, Accuracy: 82.723921%\n",
      "Epoch 89/200, Loss: 0.5529, Accuracy: 82.504732%\n",
      "Epoch 90/200, Loss: 0.5569, Accuracy: 82.474843%\n",
      "Epoch 91/200, Loss: 0.5565, Accuracy: 82.549567%\n",
      "Epoch 92/200, Loss: 0.5565, Accuracy: 82.763774%\n",
      "Epoch 93/200, Loss: 0.5534, Accuracy: 82.554548%\n",
      "Epoch 94/200, Loss: 0.5616, Accuracy: 82.574474%\n",
      "Epoch 95/200, Loss: 0.5613, Accuracy: 82.230746%\n",
      "Epoch 96/200, Loss: 0.5591, Accuracy: 82.504732%\n",
      "Epoch 97/200, Loss: 0.5594, Accuracy: 82.474843%\n",
      "Epoch 98/200, Loss: 0.5556, Accuracy: 82.544585%\n",
      "Epoch 99/200, Loss: 0.5568, Accuracy: 82.459898%\n",
      "Epoch 100/200, Loss: 0.5594, Accuracy: 82.554548%\n",
      "Epoch 101/200, Loss: 0.5657, Accuracy: 82.380193%\n",
      "Epoch 102/200, Loss: 0.5571, Accuracy: 82.529640%\n",
      "Epoch 103/200, Loss: 0.5617, Accuracy: 82.479825%\n",
      "Epoch 104/200, Loss: 0.5613, Accuracy: 82.439972%\n",
      "Epoch 105/200, Loss: 0.5640, Accuracy: 82.444954%\n",
      "Epoch 106/200, Loss: 0.5713, Accuracy: 82.156023%\n",
      "Epoch 107/200, Loss: 0.5616, Accuracy: 82.504732%\n",
      "Epoch 108/200, Loss: 0.5577, Accuracy: 82.504732%\n",
      "Epoch 109/200, Loss: 0.5571, Accuracy: 82.589419%\n",
      "Epoch 110/200, Loss: 0.5828, Accuracy: 82.066354%\n",
      "Epoch 111/200, Loss: 0.5627, Accuracy: 82.385175%\n",
      "Epoch 112/200, Loss: 0.5578, Accuracy: 82.604364%\n",
      "Epoch 113/200, Loss: 0.5587, Accuracy: 82.519677%\n",
      "Epoch 114/200, Loss: 0.5564, Accuracy: 82.808608%\n",
      "Epoch 115/200, Loss: 0.5565, Accuracy: 82.489788%\n",
      "Epoch 116/200, Loss: 0.5556, Accuracy: 82.748829%\n",
      "Epoch 117/200, Loss: 0.5636, Accuracy: 82.420046%\n",
      "Epoch 118/200, Loss: 0.5596, Accuracy: 82.679087%\n",
      "Epoch 119/200, Loss: 0.5592, Accuracy: 82.664143%\n",
      "Epoch 120/200, Loss: 0.5598, Accuracy: 82.390156%\n",
      "Epoch 121/200, Loss: 0.5553, Accuracy: 82.574474%\n",
      "Epoch 122/200, Loss: 0.5529, Accuracy: 82.763774%\n",
      "Epoch 123/200, Loss: 0.5585, Accuracy: 82.310451%\n",
      "Epoch 124/200, Loss: 0.5643, Accuracy: 82.205838%\n",
      "Epoch 125/200, Loss: 0.5605, Accuracy: 82.444954%\n",
      "Epoch 126/200, Loss: 0.5685, Accuracy: 82.180931%\n",
      "Epoch 127/200, Loss: 0.5599, Accuracy: 82.599382%\n",
      "Epoch 128/200, Loss: 0.5655, Accuracy: 82.275580%\n",
      "Epoch 129/200, Loss: 0.5618, Accuracy: 82.534622%\n",
      "Epoch 130/200, Loss: 0.5611, Accuracy: 82.484806%\n",
      "Epoch 131/200, Loss: 0.5589, Accuracy: 82.634253%\n",
      "Epoch 132/200, Loss: 0.5621, Accuracy: 82.420046%\n",
      "Epoch 133/200, Loss: 0.5626, Accuracy: 82.415064%\n",
      "Epoch 134/200, Loss: 0.5737, Accuracy: 82.161004%\n",
      "Epoch 135/200, Loss: 0.5655, Accuracy: 82.190894%\n",
      "Epoch 136/200, Loss: 0.5667, Accuracy: 82.340341%\n",
      "Epoch 137/200, Loss: 0.5664, Accuracy: 82.439972%\n",
      "Epoch 138/200, Loss: 0.5628, Accuracy: 82.380193%\n",
      "Epoch 139/200, Loss: 0.5636, Accuracy: 82.355285%\n",
      "Epoch 140/200, Loss: 0.5606, Accuracy: 82.619309%\n",
      "Epoch 141/200, Loss: 0.5625, Accuracy: 82.415064%\n",
      "Epoch 142/200, Loss: 0.5671, Accuracy: 82.449935%\n",
      "Epoch 143/200, Loss: 0.5614, Accuracy: 82.405101%\n",
      "Epoch 144/200, Loss: 0.5600, Accuracy: 82.390156%\n",
      "Epoch 145/200, Loss: 0.5689, Accuracy: 82.330378%\n",
      "Epoch 146/200, Loss: 0.5615, Accuracy: 82.260636%\n",
      "Epoch 147/200, Loss: 0.5603, Accuracy: 82.375212%\n",
      "Epoch 148/200, Loss: 0.5603, Accuracy: 82.504732%\n",
      "Epoch 149/200, Loss: 0.5664, Accuracy: 82.489788%\n",
      "Epoch 150/200, Loss: 0.5606, Accuracy: 82.504732%\n",
      "Epoch 151/200, Loss: 0.5619, Accuracy: 82.444954%\n",
      "Epoch 152/200, Loss: 0.5592, Accuracy: 82.434991%\n",
      "Epoch 153/200, Loss: 0.5593, Accuracy: 82.514696%\n",
      "Epoch 154/200, Loss: 0.5725, Accuracy: 82.539603%\n",
      "Epoch 155/200, Loss: 0.5635, Accuracy: 82.584438%\n",
      "Epoch 156/200, Loss: 0.5624, Accuracy: 82.594401%\n",
      "Epoch 157/200, Loss: 0.5599, Accuracy: 82.449935%\n",
      "Epoch 158/200, Loss: 0.5560, Accuracy: 82.180931%\n",
      "Epoch 159/200, Loss: 0.5549, Accuracy: 82.544585%\n",
      "Epoch 160/200, Loss: 0.5687, Accuracy: 82.275580%\n",
      "Epoch 161/200, Loss: 0.5630, Accuracy: 82.190894%\n",
      "Epoch 162/200, Loss: 0.5600, Accuracy: 82.534622%\n",
      "Epoch 163/200, Loss: 0.5643, Accuracy: 82.225765%\n",
      "Epoch 164/200, Loss: 0.5558, Accuracy: 82.694032%\n",
      "Epoch 165/200, Loss: 0.5621, Accuracy: 82.370230%\n",
      "Epoch 166/200, Loss: 0.5592, Accuracy: 82.614327%\n",
      "Epoch 167/200, Loss: 0.5631, Accuracy: 82.439972%\n",
      "Epoch 168/200, Loss: 0.5652, Accuracy: 82.260636%\n",
      "Epoch 169/200, Loss: 0.5670, Accuracy: 82.141078%\n",
      "Epoch 170/200, Loss: 0.5611, Accuracy: 82.524659%\n",
      "Epoch 171/200, Loss: 0.5557, Accuracy: 82.718940%\n",
      "Epoch 172/200, Loss: 0.5585, Accuracy: 82.579456%\n",
      "Epoch 173/200, Loss: 0.5659, Accuracy: 82.131115%\n",
      "Epoch 174/200, Loss: 0.5610, Accuracy: 82.275580%\n",
      "Epoch 175/200, Loss: 0.5645, Accuracy: 82.315433%\n",
      "Epoch 176/200, Loss: 0.5640, Accuracy: 82.330378%\n",
      "Epoch 177/200, Loss: 0.5642, Accuracy: 82.265617%\n",
      "Epoch 178/200, Loss: 0.5597, Accuracy: 82.385175%\n",
      "Epoch 179/200, Loss: 0.5577, Accuracy: 82.574474%\n",
      "Epoch 180/200, Loss: 0.5656, Accuracy: 82.330378%\n",
      "Epoch 181/200, Loss: 0.5620, Accuracy: 82.370230%\n",
      "Epoch 182/200, Loss: 0.5692, Accuracy: 82.235728%\n",
      "Epoch 183/200, Loss: 0.5687, Accuracy: 82.385175%\n",
      "Epoch 184/200, Loss: 0.5691, Accuracy: 82.121152%\n",
      "Epoch 185/200, Loss: 0.5754, Accuracy: 82.180931%\n",
      "Epoch 186/200, Loss: 0.5727, Accuracy: 81.961742%\n",
      "Epoch 187/200, Loss: 0.5699, Accuracy: 82.225765%\n",
      "Epoch 188/200, Loss: 0.5655, Accuracy: 82.190894%\n",
      "Epoch 189/200, Loss: 0.5651, Accuracy: 82.504732%\n",
      "Epoch 190/200, Loss: 0.5660, Accuracy: 82.469862%\n",
      "Epoch 191/200, Loss: 0.5644, Accuracy: 82.489788%\n",
      "Epoch 192/200, Loss: 0.5620, Accuracy: 82.200857%\n",
      "Epoch 193/200, Loss: 0.5599, Accuracy: 82.444954%\n",
      "Epoch 194/200, Loss: 0.5644, Accuracy: 82.390156%\n",
      "Epoch 195/200, Loss: 0.5605, Accuracy: 82.430009%\n",
      "Epoch 196/200, Loss: 0.5651, Accuracy: 82.444954%\n",
      "Epoch 197/200, Loss: 0.5707, Accuracy: 82.180931%\n",
      "Epoch 198/200, Loss: 0.5697, Accuracy: 82.230746%\n",
      "Epoch 199/200, Loss: 0.5661, Accuracy: 82.205838%\n",
      "Epoch 200/200, Loss: 0.5635, Accuracy: 82.185912%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# 최적화 알고리즘\n",
    "optimizer = Adam(rnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 손실 함수\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 모델 학습 설정\n",
    "rnn_model.train()\n",
    "\n",
    "\n",
    "# 학습 횟수만큼 반복\n",
    "for epoch in range(epochs):\n",
    "    # 데이터 셔플 - reference : https://blog.naver.com/frogsom1120/222127699322\n",
    "    shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # 데이터 분할은 하지 않음.\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # 데이터 (rows) 학습 \n",
    "    for index, row in shuffled_df.iterrows():\n",
    "        # 이름을 텐서로 변환 (one-hot encoding)\n",
    "        input_tensor = name_to_tensor(row['Name'])\n",
    "        # 국적을 텐서로 변환\n",
    "        target_tensor = torch.tensor([country_list.index(row['Country'])], dtype=torch.long)\n",
    "\n",
    "        # 모델 은닉층(상태)를 얻어옴\n",
    "        hidden = rnn_model.get_hidden()\n",
    "\n",
    "        # 모델 그레디언트 초기화\n",
    "        rnn_model.zero_grad()\n",
    "\n",
    "        # rnn 학습\n",
    "        for char_index in range(input_tensor.size(0)):\n",
    "            # char tensor 추출 : 2차원 텐서 (1, 28)\n",
    "            char_tensor = input_tensor[char_index]\n",
    "            # name char 학습 : 1차원 텐서 (28)\n",
    "            output, hidden = rnn_model(char_tensor[None, :], hidden)\n",
    "\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = loss_fn(output, target_tensor)\n",
    "        # 손실 역전파\n",
    "        loss.backward()\n",
    "        # 최적화 실행\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 합계 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 예측 결과 계산\n",
    "        predicted_index = torch.argmax(output, dim=1)\n",
    "\n",
    "        # 예측 결과 확인\n",
    "        correct_predictions += (predicted_index == target_tensor).sum().item()\n",
    "        total_predictions += 1\n",
    "\n",
    "\n",
    "    # 평균 손실 계산\n",
    "    avg_loss = total_loss / total_predictions\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "    # 학습 횟수 출력\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:2f}%\")\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 18 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m     char_tensor = test_tensor[char_index]\n\u001b[32m     10\u001b[39m     output, hidden = rnn_model(char_tensor[\u001b[38;5;28;01mNone\u001b[39;00m, :], hidden)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 예측 결과 확인\u001b[39;00m\n\u001b[32m     15\u001b[39m predicted_index = torch.argmax(output, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: a Tensor with 18 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "test_name = 'jinping'\n",
    "test_tensor = name_to_tensor(test_name)\n",
    "\n",
    "rnn_model.eval()\n",
    "\n",
    "hidden = rnn_model.get_hidden()\n",
    "\n",
    "for char_index in range(test_tensor.size(0)):\n",
    "    char_tensor = test_tensor[char_index]\n",
    "    output, hidden = rnn_model(char_tensor[None, :], hidden)\n",
    "\n",
    "\n",
    "print (f\"Output : {output.item()}\")\n",
    "# 예측 결과 확인\n",
    "predicted_index = torch.argmax(output, dim=1)\n",
    "print(country_list[predicted_index.item()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
